The main difference between each individual reference is that sometimes they use different words for the one word in the candidate or they sometimes use a different structure of grammers so some words are switched.

When we are using n = 3 in BLEU evalution, some of the scores are becoming 0. More spefically in the case where we are building the AM model with 30K sentences, we have seven 0's out of twenty-five test sentences. However, we are having much better scores with using n = 2 ranging from 0.26968 to 1 (we got one french sentence "Nous vivons dans une democratie." that is perfectly translated to the exact corresponding english sentence in Task5.e). It seems to be better with n = 2 since having three consecutive grams matching with the reference is hard to achive and n = 1 doesn't seem to be enough as discussed in the slides 6-2. 